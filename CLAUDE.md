# Linux ARM KVMé‚®ä»¶åˆ—è¡¨è‡ªåŠ¨åŒ–åˆ†æç³»ç»Ÿ

## ç³»ç»Ÿæ¦‚è¿°

æ„å»ºä¸€ä¸ªè‡ªåŠ¨åŒ–çš„Linux ARM KVMé‚®ä»¶åˆ—è¡¨åˆ†æç³»ç»Ÿï¼Œæ”¯æŒå®šæœŸæŠ“å–ã€è§£æã€åˆ†æå’Œå¯è§†åŒ–é‚®ä»¶çº¿ç¨‹ã€‚

## æ ¸å¿ƒåŠŸèƒ½

### 1. è‡ªåŠ¨ä»“åº“ç®¡ç†
- **åˆå§‹ä¸‹è½½**: `git clone --mirror https://lore.kernel.org/kvmarm/0 kvmarm/git/0.git`
- **å¢é‡æ›´æ–°**: æ£€æµ‹ç°æœ‰ä»“åº“ï¼Œæ‰§è¡Œ `git remote update` è·å–æœ€æ–°é‚®ä»¶
- **æ—¶é—´èŒƒå›´è¿‡æ»¤**: æ”¯æŒæŒ‡å®šæ—¶é—´èŒƒå›´çš„é‚®ä»¶æå–ï¼ˆå¦‚ï¼šä¸Šå‘¨åˆ°å½“å‰æ—¶é—´ï¼‰

### 2. ä¸¤é˜¶æ®µåˆ†ææµç¨‹
- **é˜¶æ®µ1**: Gitä»“åº“è§£æ â†’ JSONæ ‘å½¢ç»“æ„ + ASCIIå¯è§†åŒ–
- **é˜¶æ®µ2**: æ™ºèƒ½å†…å®¹åˆ†å‰² â†’ AIåˆ†æ â†’ çº¿ç¨‹æ€»ç»“

### 3. Lore.kernel.orgé“¾æ¥ç”Ÿæˆ
- **åå‘é“¾æ¥**: ä»Message-IDç”Ÿæˆlore.kernel.orgé“¾æ¥
- **URLç®—æ³•**: åŸºäºMessage-IDçš„æ ‡å‡†åŒ–URLæ„å»º
- **å¯ç‚¹å‡»å¯¼èˆª**: åœ¨è¾“å‡ºä¸­æä¾›ç›´æ¥è·³è½¬é“¾æ¥
### 4. AIæ¨¡å‹æ”¯æŒ
- **é»˜è®¤**: OpenAI GPT-4
- **å¯é€‰**: Google Gemini, Anthropic Claude, æˆ–å…¶ä»–å…¼å®¹API
- **é…ç½®**: é€šè¿‡ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶åˆ‡æ¢

### 5. è‡ªåŠ¨åŒ–è°ƒåº¦
- **æœ¬åœ°æµ‹è¯•**: ä¼˜å…ˆå®Œæˆæœ¬åœ°åŠŸèƒ½éªŒè¯
- **GitHub Actions**: æœ¬åœ°æµ‹è¯•å®Œæˆåå†å®ç°è‡ªåŠ¨åŒ–
- **å¢é‡åˆ†æ**: åªå¤„ç†æ–°å¢æˆ–æ›´æ–°çš„é‚®ä»¶
- **ç»“æœå­˜å‚¨**: è‡ªåŠ¨æäº¤åˆ†æç»“æœåˆ°ä»“åº“

## æ ¸å¿ƒæ•°æ®ç»“æ„

```python
@dataclass
class EmailNode:
    git_hash: str                    # Git commit hash
    message_id: str                  # é‚®ä»¶ID
    subject: str                     # ä¸»é¢˜
    sender: str                      # å‘é€è€…
    date: datetime                   # æ—¶é—´
    message_type: str                # patch/reply/review/ack
    parent_id: Optional[str]         # çˆ¶é‚®ä»¶ID
    children_ids: List[str]          # å­é‚®ä»¶IDåˆ—è¡¨
    
    # Lore.kernel.orgé“¾æ¥å’ŒéªŒè¯
    lore_url: str                    # ç”Ÿæˆçš„loreé“¾æ¥
    lore_validation: Optional[Dict]   # atoméªŒè¯ç»“æœ
    
    # ARM KVMç‰¹æœ‰
    is_arm_kvm_related: bool
    patch_info: Optional[PatchInfo]
    
    # åˆ†æç»“æœ
    ai_analysis: Optional[Dict]
    content_chunks: List[ContentChunk]

@dataclass
class EmailThread:
    thread_id: str
    root_node: EmailNode
    all_nodes: Dict[str, EmailNode]
    statistics: ThreadStats
    ai_summary: Optional[Dict]
```

## æ™ºèƒ½åˆ†å‰²ç­–ç•¥

### åˆ†å‰²æ¡ä»¶
- Tokenæ•°é‡ > 8000
- ä»£ç diff > 10KB
- å¤šå±‚å›å¤åµŒå¥— > 3çº§

### åˆ†å—ç±»å‹
```python
class ChunkType(Enum):
    HEADER = "header"              # ä¼˜å…ˆçº§: 5
    SUMMARY = "summary"            # ä¼˜å…ˆçº§: 4
    CODE_CRITICAL = "code_critical" # ä¼˜å…ˆçº§: 4
    DISCUSSION = "discussion"       # ä¼˜å…ˆçº§: 3
    CODE_DETAIL = "code_detail"    # ä¼˜å…ˆçº§: 2
    METADATA = "metadata"          # ä¼˜å…ˆçº§: 1
```

## Lore.kernel.orgé“¾æ¥ç”Ÿæˆå’ŒéªŒè¯ç®—æ³•

### Message-IDåˆ°URLæ˜ å°„
```python
def generate_lore_url(message_id: str, mailing_list: str = "kvmarm") -> str:
    """
    ä»Message-IDç”Ÿæˆlore.kernel.orgé“¾æ¥
    
    ç®—æ³•åŸç†ï¼š
    1. lore.kernel.orgä½¿ç”¨Message-IDä½œä¸ºå”¯ä¸€æ ‡è¯†
    2. URLæ ¼å¼: https://lore.kernel.org/{list}/{message_id}/
    3. Message-IDéœ€è¦URLç¼–ç å¤„ç†ç‰¹æ®Šå­—ç¬¦
    
    ç¤ºä¾‹ï¼š
    Message-ID: <20250705071717.5062-1-ankita@nvidia.com>
    URL: https://lore.kernel.org/kvmarm/20250705071717.5062-1-ankita@nvidia.com/
    """
    
    # ç§»é™¤å°–æ‹¬å·
    clean_id = message_id.strip('<>')
    
    # URLç¼–ç ç‰¹æ®Šå­—ç¬¦
    encoded_id = urllib.parse.quote(clean_id, safe='@.-_')
    
    # æ„å»ºlore URL
    lore_url = f"https://lore.kernel.org/{mailing_list}/{encoded_id}/"
    
    return lore_url

def validate_lore_url_with_atom(url: str, email_node: EmailNode) -> Dict[str, Any]:
    """
    ä½¿ç”¨atom feedéªŒè¯loreé“¾æ¥å¹¶åŒ¹é…å†…å®¹
    
    éªŒè¯ç­–ç•¥ï¼š
    1. è·å–é‚®ä»¶çš„atom feed
    2. è§£æatomå†…å®¹æå–å…³é”®ä¿¡æ¯
    3. ä¸æœ¬åœ°é‚®ä»¶å†…å®¹è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
    4. è¿”å›éªŒè¯ç»“æœå’ŒåŒ¹é…åº¦
    """
    
    try:
        # æ„å»ºatom feed URL
        atom_url = url.rstrip('/') + '/raw'
        
        # è·å–atomå†…å®¹
        response = requests.get(atom_url, timeout=10, headers={
            'User-Agent': 'ARM-KVM-Analyzer/1.0'
        })
        
        if response.status_code != 200:
            return {
                "valid": False,
                "error": f"HTTP {response.status_code}",
                "match_score": 0
            }
        
        # è§£æatomå†…å®¹
        atom_content = response.text
        atom_data = parse_atom_email_content(atom_content)
        
        # ä¸æœ¬åœ°é‚®ä»¶è¿›è¡ŒåŒ¹é…éªŒè¯
        match_result = fuzzy_match_email_content(atom_data, email_node)
        
        return {
            "valid": True,
            "match_score": match_result["score"],
            "matched_fields": match_result["matched_fields"],
            "atom_data": atom_data,
            "verification_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {
            "valid": False,
            "error": str(e),
            "match_score": 0
        }

def parse_atom_email_content(atom_content: str) -> Dict[str, str]:
    """
    è§£æatomæ ¼å¼çš„é‚®ä»¶å†…å®¹
    æå–å…³é”®å­—æ®µç”¨äºåŒ¹é…
    """
    
    # ä½¿ç”¨email parserè§£æåŸå§‹é‚®ä»¶å†…å®¹
    try:
        email_msg = email.message_from_string(atom_content)
        
        return {
            "subject": email_msg.get("Subject", "").strip(),
            "from": email_msg.get("From", "").strip(),
            "date": email_msg.get("Date", "").strip(),
            "message_id": email_msg.get("Message-ID", "").strip('<>'),
            "content_preview": get_email_content_preview(email_msg, 500)
        }
    except Exception as e:
        # å¦‚æœemailè§£æå¤±è´¥ï¼Œå°è¯•æ­£åˆ™è¡¨è¾¾å¼æå–
        return extract_email_fields_with_regex(atom_content)

def fuzzy_match_email_content(atom_data: Dict[str, str], email_node: EmailNode) -> Dict[str, Any]:
    """
    æ¨¡ç³ŠåŒ¹é…atomæ•°æ®å’Œæœ¬åœ°é‚®ä»¶æ•°æ®
    
    åŒ¹é…ç­–ç•¥ï¼š
    1. Message-IDç²¾ç¡®åŒ¹é… (æƒé‡: 40%)
    2. Subjectç›¸ä¼¼åº¦åŒ¹é… (æƒé‡: 30%)
    3. å‘é€è€…åŒ¹é… (æƒé‡: 20%)
    4. å†…å®¹é¢„è§ˆåŒ¹é… (æƒé‡: 10%)
    """
    
    match_scores = {}
    matched_fields = []
    
    # 1. Message-IDåŒ¹é… (æœ€é‡è¦)
    if atom_data.get("message_id") == email_node.message_id:
        match_scores["message_id"] = 1.0
        matched_fields.append("message_id")
    else:
        match_scores["message_id"] = 0.0
    
    # 2. Subjectç›¸ä¼¼åº¦åŒ¹é…
    atom_subject = normalize_subject_for_matching(atom_data.get("subject", ""))
    local_subject = normalize_subject_for_matching(email_node.subject)
    
    subject_similarity = calculate_string_similarity(atom_subject, local_subject)
    match_scores["subject"] = subject_similarity
    
    if subject_similarity > 0.8:
        matched_fields.append("subject")
    
    # 3. å‘é€è€…åŒ¹é…
    atom_sender = extract_email_address(atom_data.get("from", ""))
    local_sender = extract_email_address(email_node.sender)
    
    if atom_sender and local_sender and atom_sender.lower() == local_sender.lower():
        match_scores["sender"] = 1.0
        matched_fields.append("sender")
    else:
        sender_similarity = calculate_string_similarity(atom_sender, local_sender)
        match_scores["sender"] = sender_similarity
    
    # 4. æ—¥æœŸåŒ¹é… (å¯é€‰)
    if atom_data.get("date") and email_node.date:
        date_match = compare_email_dates(atom_data["date"], email_node.date)
        match_scores["date"] = date_match
        if date_match > 0.9:
            matched_fields.append("date")
    
    # è®¡ç®—æ€»ä½“åŒ¹é…åˆ†æ•°
    weights = {
        "message_id": 0.4,
        "subject": 0.3, 
        "sender": 0.2,
        "date": 0.1
    }
    
    total_score = sum(
        match_scores.get(field, 0) * weight 
        for field, weight in weights.items()
    )
    
    return {
        "score": total_score,
        "matched_fields": matched_fields,
        "individual_scores": match_scores,
        "confidence": "high" if total_score > 0.8 else "medium" if total_score > 0.6 else "low"
    }

def calculate_string_similarity(str1: str, str2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦"""
    if not str1 or not str2:
        return 0.0
    
    # ä½¿ç”¨å¤šç§ç›¸ä¼¼åº¦ç®—æ³•
    from difflib import SequenceMatcher
    
    # åŸºæœ¬ç›¸ä¼¼åº¦
    basic_sim = SequenceMatcher(None, str1.lower(), str2.lower()).ratio()
    
    # å»é™¤å¸¸è§å‰ç¼€åçš„ç›¸ä¼¼åº¦ï¼ˆå¦‚Re:, [PATCH]ç­‰ï¼‰
    clean_str1 = re.sub(r'^(re:|fwd:|\[patch[^\]]*\])\s*', '', str1.lower())
    clean_str2 = re.sub(r'^(re:|fwd:|\[patch[^\]]*\])\s*', '', str2.lower())
    clean_sim = SequenceMatcher(None, clean_str1, clean_str2).ratio()
    
    # è¿”å›è¾ƒé«˜çš„ç›¸ä¼¼åº¦
    return max(basic_sim, clean_sim)

def normalize_subject_for_matching(subject: str) -> str:
    """æ ‡å‡†åŒ–ä¸»é¢˜ç”¨äºåŒ¹é…"""
    # ç§»é™¤å¸¸è§çš„å‰ç¼€å’Œåç¼€
    normalized = re.sub(r'^(re:|fwd:)\s*', '', subject.lower())
    normalized = re.sub(r'\[patch[^\]]*\]\s*', '', normalized)
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    return normalized

def extract_email_address(from_field: str) -> str:
    """ä»Fromå­—æ®µæå–çº¯é‚®ç®±åœ°å€"""
    import re
    
    # åŒ¹é…é‚®ç®±åœ°å€
    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
    match = re.search(email_pattern, from_field)
    
    return match.group(1) if match else ""

def batch_validate_lore_urls(email_nodes: List[EmailNode], max_workers: int = 5) -> Dict[str, Dict]:
    """
    æ‰¹é‡éªŒè¯loreé“¾æ¥
    ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘éªŒè¯ä»¥æé«˜æ•ˆç‡
    """
    
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import time
    
    validation_results = {}
    
    def validate_single_email(email_node):
        """éªŒè¯å•ä¸ªé‚®ä»¶çš„loreé“¾æ¥"""
        try:
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(0.5)
            
            result = validate_lore_url_with_atom(email_node.lore_url, email_node)
            return email_node.message_id, result
        except Exception as e:
            return email_node.message_id, {
                "valid": False,
                "error": f"Validation failed: {str(e)}",
                "match_score": 0
            }
    
    print(f"ğŸ” å¼€å§‹æ‰¹é‡éªŒè¯ {len(email_nodes)} ä¸ªloreé“¾æ¥...")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰éªŒè¯ä»»åŠ¡
        future_to_email = {
            executor.submit(validate_single_email, email_node): email_node
            for email_node in email_nodes
        }
        
        # æ”¶é›†éªŒè¯ç»“æœ
        for i, future in enumerate(as_completed(future_to_email)):
            message_id, result = future.result()
            validation_results[message_id] = result
            
            # æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 10 == 0:
                print(f"   å·²éªŒè¯: {i + 1}/{len(email_nodes)}")
    
    # ç»Ÿè®¡éªŒè¯ç»“æœ
    valid_count = sum(1 for r in validation_results.values() if r["valid"])
    high_confidence = sum(1 for r in validation_results.values() 
                         if r.get("match_score", 0) > 0.8)
    
    print(f"âœ… éªŒè¯å®Œæˆ: {valid_count}/{len(email_nodes)} æœ‰æ•ˆ")
    print(f"ğŸ¯ é«˜ç½®ä¿¡åº¦åŒ¹é…: {high_confidence}/{len(email_nodes)}")
    
    return validation_results
```

## æ ¸å¿ƒç®—æ³•ï¼ˆæ›´æ–°ç‰ˆï¼‰

```python
def main_pipeline(date_range: Optional[Tuple[datetime, datetime]] = None, debug: bool = True):
    """ä¸»å¤„ç†æµç¨‹ - æœ¬åœ°æµ‹è¯•ä¼˜åŒ–ç‰ˆ"""
    
    if debug:
        setup_debug_logging()
    
    # Step 1: ä»“åº“ç®¡ç†
    print("ğŸ“‚ Step 1: ä»“åº“ç®¡ç†...")
    repo_path = ensure_repository_updated()
    
    # Step 2: é‚®ä»¶æå–å’Œè§£æ  
    print("ğŸ“§ Step 2: é‚®ä»¶æå–...")
    emails = extract_emails_by_date_range(repo_path, date_range)
    print(f"   æå–åˆ° {len(emails)} å°é‚®ä»¶")
    
    # Step 3: æ„å»ºæ ‘å½¢ç»“æ„
    print("ğŸŒ³ Step 3: æ„å»ºæ ‘å½¢ç»“æ„...")
    forest = build_email_forest(emails)
    print(f"   æ„å»ºäº† {len(forest.threads)} ä¸ªçº¿ç¨‹")
    
    # Step 4: ç”Ÿæˆloreé“¾æ¥
    print("ğŸ”— Step 4: ç”Ÿæˆloreé“¾æ¥...")
    add_lore_links(forest)
    
    # Step 5: éªŒè¯loreé“¾æ¥
    print("ğŸ” Step 5: éªŒè¯loreé“¾æ¥...")
    validate_lore_links(forest, debug=debug)
    
    # Step 6: æ™ºèƒ½åˆ†å‰²
    print("âœ‚ï¸  Step 6: æ™ºèƒ½å†…å®¹åˆ†å‰²...")
    chunked_data = apply_intelligent_chunking(forest)
    
    # Step 7: AIåˆ†æ
    print("ğŸ¤– Step 7: AIåˆ†æ...")
    analyses = analyze_with_ai(chunked_data, debug=debug)
    
    # Step 8: ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“Š Step 8: ç”ŸæˆæŠ¥å‘Š...")
    generate_reports(forest, analyses)
    
    print("âœ… åˆ†æå®Œæˆï¼")

def add_lore_links(forest: EmailForest):
    """ä¸ºæ‰€æœ‰é‚®ä»¶æ·»åŠ loreé“¾æ¥"""
    for thread in forest.threads:
        for node in thread.all_nodes.values():
            node.lore_url = generate_lore_url(node.message_id)

def validate_lore_links(forest: EmailForest, debug: bool = False):
    """éªŒè¯æ‰€æœ‰loreé“¾æ¥"""
    all_nodes = []
    for thread in forest.threads:
        all_nodes.extend(thread.all_nodes.values())
    
    # æ‰¹é‡éªŒè¯
    validation_results = batch_validate_lore_urls(all_nodes, max_workers=3)
    
    # å°†éªŒè¯ç»“æœæ·»åŠ åˆ°èŠ‚ç‚¹
    for thread in forest.threads:
        for node in thread.all_nodes.values():
            node.lore_validation = validation_results.get(node.message_id)
            
            if debug and node.lore_validation:
                match_score = node.lore_validation.get("match_score", 0)
                if match_score < 0.6:
                    print(f"âš ï¸  ä½åŒ¹é…åº¦: {node.subject[:50]}... (åˆ†æ•°: {match_score:.2f})")

def extract_emails_by_date_range(repo_path: str, date_range: Optional[Tuple]) -> List[EmailNode]:
    """æŒ‰æ—¶é—´èŒƒå›´æå–é‚®ä»¶ - å¢åŠ loreé“¾æ¥ç”Ÿæˆ"""
    if date_range:
        since, until = date_range
        commits = get_commits_in_range(repo_path, since, until)
    else:
        # æœ¬åœ°æµ‹è¯•ï¼šåªå–æœ€è¿‘100ä¸ªcommits
        commits = get_recent_commits(repo_path, limit=100)
    
    emails = []
    for commit in commits:
        email = parse_commit_to_email(commit)
        email.lore_url = generate_lore_url(email.message_id)
        emails.append(email)
    
    return emails
```

def apply_intelligent_chunking(forest: EmailForest) -> Dict[str, List[ContentChunk]]:
    """æ™ºèƒ½å†…å®¹åˆ†å‰²"""
    chunked_data = {}
    for thread in forest.threads:
        for node in thread.all_nodes.values():
            content = extract_full_content(node.git_hash)
            if needs_chunking(content):
                chunks = smart_chunk_content(content, node.message_type)
            else:
                chunks = [ContentChunk(content=content, type=ChunkType.HEADER, priority=5)]
            chunked_data[node.message_id] = chunks
    return chunked_data
```

## AIåˆ†ææ¥å£

```python
class AIAnalyzer:
    def __init__(self, provider: str = "openai"):
        self.provider = provider
        self.client = self._init_client()
    
    def analyze_chunk(self, chunk: ContentChunk, context: Dict) -> Dict:
        """åˆ†æå•ä¸ªå†…å®¹å—"""
        prompt = self._build_prompt(chunk, context)
        return self._call_api(prompt)
    
    def merge_analyses(self, chunk_analyses: List[Dict]) -> Dict:
        """åˆå¹¶åˆ†å—åˆ†æç»“æœ"""
        merge_prompt = self._build_merge_prompt(chunk_analyses)
        return self._call_api(merge_prompt)
    
    def _init_client(self):
        if self.provider == "openai":
            return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        elif self.provider == "gemini":
            return genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        # æ·»åŠ å…¶ä»–æä¾›å•†...
```

## ç¯å¢ƒé…ç½®

### æœ¬åœ°å¼€å‘ç¯å¢ƒ
```bash
# ä½¿ç”¨å·²é…ç½®çš„ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="$OPENAI_API_KEY"  # ä½¿ç”¨ç°æœ‰é…ç½®
export AI_PROVIDER="openai"               # é»˜è®¤ä½¿ç”¨OpenAI
export MAX_TOKENS_PER_CHUNK=8000
```

### æœ¬åœ°æµ‹è¯•ä¼˜å…ˆ
- **å¼€å‘ç­–ç•¥**: å…ˆå®Œæˆæœ¬åœ°æµ‹è¯•ï¼ŒéªŒè¯æ‰€æœ‰åŠŸèƒ½æ­£å¸¸åå†å®ç°GitHub Actions
- **æµ‹è¯•æ•°æ®**: ä½¿ç”¨æœ€è¿‘1-2å‘¨çš„é‚®ä»¶æ•°æ®è¿›è¡Œæµ‹è¯•
- **è°ƒè¯•æ¨¡å¼**: æ”¯æŒè¯¦ç»†æ—¥å¿—è¾“å‡ºå’Œä¸­é—´ç»“æœä¿å­˜

## è¿­ä»£å¼€å‘æ­¥éª¤ï¼ˆæœ¬åœ°æµ‹è¯•ç‰ˆï¼‰

### Step 1: åŸºç¡€ä»“åº“ç®¡ç†
```python
def test_repository_management():
    """æµ‹è¯•ä»“åº“ä¸‹è½½å’Œæ›´æ–°"""
    print("ğŸ§ª æµ‹è¯•ä»“åº“ç®¡ç†...")
    
    # æ£€æŸ¥ä»“åº“æ˜¯å¦å­˜åœ¨
    repo_path = "kvmarm/git/0.git"
    if os.path.exists(repo_path):
        print("âœ… ä»“åº“å·²å­˜åœ¨ï¼Œæ‰§è¡Œæ›´æ–°...")
        result = run_command("git remote update", cwd=repo_path)
    else:
        print("ğŸ“¥ ä¸‹è½½ä»“åº“...")
        result = run_command("git clone --mirror https://lore.kernel.org/kvmarm/0 kvmarm/git/0.git")
    
    assert os.path.exists(repo_path)
    assert is_git_repository(repo_path)
    print("âœ… ä»“åº“ç®¡ç†æµ‹è¯•é€šè¿‡")

# è¿è¡Œ: python -c "from analyze import test_repository_management; test_repository_management()"
```

### Step 2: é‚®ä»¶è§£æå’Œloreé“¾æ¥
```python
def test_email_parsing_with_lore():
    """æµ‹è¯•é‚®ä»¶æå–ã€è§£æå’Œloreé“¾æ¥ç”Ÿæˆ"""
    print("ğŸ§ª æµ‹è¯•é‚®ä»¶è§£æå’Œloreé“¾æ¥...")
    
    # æå–æœ€è¿‘10å°é‚®ä»¶è¿›è¡Œæµ‹è¯•
    emails = extract_emails_by_date_range("kvmarm/git/0.git", None)[:10]
    
    assert len(emails) > 0
    print(f"   æå–äº† {len(emails)} å°é‚®ä»¶")
    
    # æ£€æŸ¥åŸºæœ¬å­—æ®µ
    for email in emails:
        assert email.git_hash
        assert email.message_id
        assert email.lore_url
        print(f"   ğŸ“§ {email.subject[:50]}...")
        print(f"      ğŸ”— {email.lore_url}")
    
    # éªŒè¯loreé“¾æ¥æ ¼å¼
    sample_email = emails[0]
    expected_pattern = r"https://lore\.kernel\.org/kvmarm/[^/]+/"
    assert re.match(expected_pattern, sample_email.lore_url)
    
    print("âœ… é‚®ä»¶è§£æå’Œloreé“¾æ¥æµ‹è¯•é€šè¿‡")

# è¿è¡Œ: python -c "from analyze import test_email_parsing_with_lore; test_email_parsing_with_lore()"
```

### Step 3: Loreé“¾æ¥éªŒè¯
```python
def test_lore_link_validation():
    """æµ‹è¯•loreé“¾æ¥ç”Ÿæˆå’ŒéªŒè¯"""
    print("ğŸ§ª æµ‹è¯•loreé“¾æ¥éªŒè¯...")
    
    emails = extract_emails_by_date_range("kvmarm/git/0.git", None)[:5]  # æµ‹è¯•5å°é‚®ä»¶
    
    for email in emails:
        print(f"   ğŸ“§ æµ‹è¯•é‚®ä»¶: {email.subject[:40]}...")
        print(f"      ğŸ†” Message-ID: {email.message_id}")
        print(f"      ğŸ”— Lore URL: {email.lore_url}")
        
        # éªŒè¯é“¾æ¥
        validation_result = validate_lore_url_with_atom(email.lore_url, email)
        
        if validation_result["valid"]:
            match_score = validation_result["match_score"]
            confidence = validation_result.get("confidence", "unknown")
            matched_fields = validation_result.get("matched_fields", [])
            
            print(f"      âœ… éªŒè¯æˆåŠŸ - åŒ¹é…åº¦: {match_score:.2f} ({confidence})")
            print(f"      ğŸ¯ åŒ¹é…å­—æ®µ: {', '.join(matched_fields)}")
            
            if match_score < 0.6:
                print(f"      âš ï¸  åŒ¹é…åº¦è¾ƒä½ï¼Œè¯·æ£€æŸ¥")
        else:
            error = validation_result.get("error", "Unknown error")
            print(f"      âŒ éªŒè¯å¤±è´¥: {error}")
    
    print("âœ… Loreé“¾æ¥éªŒè¯æµ‹è¯•å®Œæˆ")

# è¿è¡Œ: python -c "from analyze import test_lore_link_validation; test_lore_link_validation()"
```

### Step 4: æ ‘å½¢ç»“æ„æ„å»º
```python
def test_tree_building():
    """æµ‹è¯•æ ‘å½¢ç»“æ„æ„å»º"""
    print("ğŸ§ª æµ‹è¯•æ ‘å½¢ç»“æ„æ„å»º...")
    
    emails = extract_emails_by_date_range("kvmarm/git/0.git", None)[:50]  # æµ‹è¯•50å°é‚®ä»¶
    forest = build_email_forest(emails)
    
    assert len(forest.threads) > 0
    print(f"   æ„å»ºäº† {len(forest.threads)} ä¸ªçº¿ç¨‹")
    
    # æ£€æŸ¥æ¯ä¸ªçº¿ç¨‹çš„å®Œæ•´æ€§
    for i, thread in enumerate(forest.threads[:3]):  # æ£€æŸ¥å‰3ä¸ªçº¿ç¨‹
        assert thread.root_node
        print(f"   ğŸ“‹ çº¿ç¨‹ {i+1}: {thread.subject[:60]}")
        print(f"      ğŸ”— {thread.root_node.lore_url}")
        print(f"      ğŸ“Š {len(thread.all_nodes)} å°é‚®ä»¶")
    
    print("âœ… æ ‘å½¢ç»“æ„æ„å»ºæµ‹è¯•é€šè¿‡")

# è¿è¡Œ: python -c "from analyze import test_tree_building; test_tree_building()"
```

### Step 5: å†…å®¹åˆ†å‰²
```python
def test_content_chunking():
    """æµ‹è¯•æ™ºèƒ½åˆ†å‰²"""
    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½å†…å®¹åˆ†å‰²...")
    
    # åˆ›å»ºæµ‹è¯•é‚®ä»¶æˆ–ä½¿ç”¨çœŸå®çš„å¤§é‚®ä»¶
    emails = extract_emails_by_date_range("kvmarm/git/0.git", None)[:20]
    forest = build_email_forest(emails)
    
    chunked_data = apply_intelligent_chunking(forest)
    
    total_chunks = sum(len(chunks) for chunks in chunked_data.values())
    chunked_emails = sum(1 for chunks in chunked_data.values() if len(chunks) > 1)
    
    print(f"   ğŸ“Š æ€»åˆ†å—æ•°: {total_chunks}")
    print(f"   ğŸ“§ éœ€è¦åˆ†å—çš„é‚®ä»¶: {chunked_emails}")
    
    # æ£€æŸ¥åˆ†å—è´¨é‡
    for message_id, chunks in list(chunked_data.items())[:3]:
        if len(chunks) > 1:
            print(f"   âœ‚ï¸  é‚®ä»¶ {message_id[:20]}... åˆ†æˆ {len(chunks)} å—")
            for chunk in chunks:
                print(f"      ğŸ“ {chunk.chunk_type} (ä¼˜å…ˆçº§: {chunk.priority})")
    
    print("âœ… å†…å®¹åˆ†å‰²æµ‹è¯•é€šè¿‡")

# è¿è¡Œ: python -c "from analyze import test_content_chunking; test_content_chunking()"
```

### Step 6: AIåˆ†ææµ‹è¯•
```python
def test_ai_analysis():
    """æµ‹è¯•AIåˆ†æåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•AIåˆ†æ...")
    
    # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„OPENAI_API_KEY
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        return
    
    analyzer = AIAnalyzer("openai")
    
    # åˆ›å»ºæµ‹è¯•åˆ†å—
    test_chunk = ContentChunk(
        chunk_id="test_001",
        git_hash="test_hash",
        chunk_type=ChunkType.HEADER,
        content="[PATCH v2 1/3] KVM: arm64: Fix memory management issue\n\nThis patch fixes a critical memory leak...",
        priority=5,
        token_count=50
    )
    
    try:
        result = analyzer.analyze_chunk(test_chunk, {"thread_subject": "Test thread"})
        
        assert "technical_points" in result
        assert "arm_kvm_relevance" in result
        
        print("   âœ… AIåˆ†æè¿”å›æ­£ç¡®æ ¼å¼")
        print(f"   ğŸ“Š ARM KVMç›¸å…³æ€§: {result.get('arm_kvm_relevance', 'N/A')}")
        
    except Exception as e:
        print(f"   âŒ AIåˆ†æå¤±è´¥: {e}")
        return
    
    print("âœ… AIåˆ†ææµ‹è¯•é€šè¿‡")

# è¿è¡Œ: python -c "from analyze import test_ai_analysis; test_ai_analysis()"
```

### Step 7: å®Œæ•´æµç¨‹æµ‹è¯•
```python
def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´æµç¨‹"""
    print("ğŸ§ª æµ‹è¯•å®Œæ•´æµç¨‹...")
    
    # ä½¿ç”¨æœ€è¿‘3å¤©çš„é‚®ä»¶è¿›è¡Œæµ‹è¯•
    end_date = datetime.now()
    start_date = end_date - timedelta(days=3)
    
    print(f"   ğŸ“… æµ‹è¯•æ—¶é—´èŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")
    
    forest, summaries = main_pipeline(
        date_range=(start_date, end_date),
        debug=True
    )
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    expected_files = [
        "arm_kvm_threads.json",
        "arm_kvm_analysis.json", 
        "chunking_statistics.json"
    ]
    
    for file_name in expected_files:
        assert os.path.exists(file_name), f"ç¼ºå°‘è¾“å‡ºæ–‡ä»¶: {file_name}"
        print(f"   âœ… ç”Ÿæˆæ–‡ä»¶: {file_name}")
    
    print("âœ… å®Œæ•´æµç¨‹æµ‹è¯•é€šè¿‡")

# è¿è¡Œ: python -c "from analyze import test_full_pipeline; test_full_pipeline()"
```

## è¾“å‡ºç¤ºä¾‹

### JSONè¾“å‡ºç»“æ„ï¼ˆåŒ…å«loreé“¾æ¥ï¼‰
```json
{
  "metadata": {
    "generated_at": "2025-01-15T10:30:00Z",
    "date_range": ["2025-01-08", "2025-01-15"],
    "repository": "kvmarm/git/0.git",
    "ai_provider": "openai",
    "total_threads": 5,
    "total_messages": 23,
    "processing_stats": {
      "chunked_emails": 3,
      "total_chunks": 12,
      "token_usage": 45230,
      "lore_links_generated": 23,
      "lore_links_verified": 21,
      "avg_match_score": 0.87
    }
  },
  "threads": [
    {
      "thread_id": "thread_20250108_001",
      "subject": "[PATCH v3 0/4] KVM: arm64: Implement lazy save/restore for SVE",
      "root_git_hash": "a1b2c3d4e5f6789",
      "root_lore_url": "https://lore.kernel.org/kvmarm/20250108091500.12345-1-developer@company.com/",
      "thread_lore_url": "https://lore.kernel.org/kvmarm/20250108091500.12345-1-developer@company.com/T/",
      "date_range": ["2025-01-08T09:15:00Z", "2025-01-10T16:45:00Z"],
      "statistics": {
        "total_messages": 8,
        "patches": 4,
        "replies": 3,
        "reviews": 1,
        "max_depth": 3,
        "contributors": [
          "developer@company.com",
          "maintainer@kernel.org", 
          "reviewer@university.edu"
        ],
        "patch_series": {
          "version": 3,
          "total_patches": 4,
          "completion_status": "under_review"
        }
      },
      "tree": {
        "node": {
          "git_hash": "a1b2c3d4e5f6789",
          "message_id": "20250108091500.12345-1-developer@company.com",
          "lore_url": "https://lore.kernel.org/kvmarm/20250108091500.12345-1-developer@company.com/",
          "subject": "[PATCH v3 0/4] KVM: arm64: Implement lazy save/restore for SVE",
          "sender": "John Developer <developer@company.com>",
          "date": "2025-01-08T09:15:00Z",
          "message_type": "patch_cover",
          "lore_validation": {
            "valid": true,
            "match_score": 0.95,
            "confidence": "high",
            "matched_fields": ["message_id", "subject", "sender"],
            "verification_timestamp": "2025-01-15T10:35:22Z"
          },
          "patch_info": {
            "version": 3,
            "number": 0,
            "total": 4,
            "series_name": "Implement lazy save/restore for SVE"
          },
          "ai_analysis": {
            "technical_complexity": "high",
            "arm_kvm_relevance": 10,
            "innovation_level": "significant",
            "key_points": [
              "Introduces lazy context switching for SVE in ARM64 KVM",
              "Performance optimization for mixed SVE/non-SVE workloads",
              "Implements trap-and-emulate mechanism for SVE register access"
            ]
          }
        },
        "children": [
          {
            "node": {
              "git_hash": "b2c3d4e5f6789a1",
              "message_id": "20250108091501.12345-2-developer@company.com",
              "lore_url": "https://lore.kernel.org/kvmarm/20250108091501.12345-2-developer@company.com/",
              "subject": "[PATCH v3 1/4] KVM: arm64: Add SVE lazy context infrastructure",
              "message_type": "patch",
              "patch_info": {"version": 3, "number": 1, "total": 4}
            },
            "children": [
              {
                "node": {
                  "git_hash": "c3d4e5f6789a1b2",
                  "message_id": "reply-001@reviewer.edu",
                  "lore_url": "https://lore.kernel.org/kvmarm/reply-001@reviewer.edu/",
                  "subject": "Re: [PATCH v3 1/4] KVM: arm64: Add SVE lazy context infrastructure",
                  "message_type": "review",
                  "reply_level": 1,
                  "ai_analysis": {
                    "review_type": "technical_deep_dive",
                    "concerns_raised": ["memory_barrier_usage", "race_condition_potential"],
                    "suggestions": ["use_rcu_protection", "add_performance_counters"],
                    "arm_kvm_relevance": 9
                  }
                },
                "children": []
              }
            ]
          }
        ]
      },
      "ai_summary": {
        "executive_summary": "This patch series implements lazy save/restore mechanism for SVE (Scalable Vector Extension) in ARM64 KVM, aiming to improve performance by deferring SVE context switches until necessary.",
        "technical_highlights": [
          "Introduces lazy SVE context switching in KVM hypervisor",
          "Reduces overhead for guests not using SVE features", 
          "Implements trap-and-emulate for SVE register access",
          "Adds performance monitoring for SVE usage patterns"
        ],
        "lore_discussion_url": "https://lore.kernel.org/kvmarm/20250108091500.12345-1-developer@company.com/T/",
        "related_threads": [
          {
            "subject": "Previous SVE discussion",
            "lore_url": "https://lore.kernel.org/kvmarm/previous-sve-thread/"
          }
        ]
      }
    }
  ]
}
```

### ASCIIå¯è§†åŒ–è¾“å‡ºï¼ˆåŒ…å«å¯ç‚¹å‡»é“¾æ¥ï¼‰
```
ğŸ“Š ARM KVM Mailing List Analysis Report
ğŸ•’ Period: 2025-01-08 to 2025-01-15 | ğŸ“§ Total: 23 messages in 5 threads
ğŸ”— Full Report: https://lore.kernel.org/kvmarm/

Thread 1: [PATCH v3 0/4] KVM: arm64: Implement lazy save/restore for SVE
ğŸ”— Thread URL: https://lore.kernel.org/kvmarm/20250108091500.12345-1-developer@company.com/T/

ğŸ“‹ Cover Letter - John Developer (2025-01-08 09:15) [a1b2c3d4] ğŸ”¥ High Impact
ğŸ”— https://lore.kernel.org/kvmarm/20250108091500.12345-1-developer@company.com/
â”œâ”€ ğŸ“„ [1/4] Add SVE lazy context infrastructure (09:16) [b2c3d4e5]
â”‚  ğŸ”— https://lore.kernel.org/kvmarm/20250108091501.12345-2-developer@company.com/
â”‚  â”œâ”€ ğŸ” Re: Technical review - Dr. Smith (10:30) [c3d4e5f6]
â”‚  â”‚  ğŸ”— https://lore.kernel.org/kvmarm/reply-001@reviewer.edu/
â”‚  â”‚  â””â”€ ğŸ’¬ Re: Response to review - John (14:45) [d4e5f6g7]
â”‚  â”‚     ğŸ”— https://lore.kernel.org/kvmarm/response-001@company.com/
â”‚  â””â”€ âœ… Re: Acked-by - Maintainer Jones (16:20) [e5f6g7h8]
â”‚     ğŸ”— https://lore.kernel.org/kvmarm/ack-001@kernel.org/
â”œâ”€ ğŸ“„ [2/4] Implement SVE trap handling (09:17) [f6g7h8i9]
â”‚  ğŸ”— https://lore.kernel.org/kvmarm/20250108091502.12345-3-developer@company.com/
â”œâ”€ ğŸ“„ [3/4] Add performance monitoring (09:18) [g7h8i9j0]
â”‚  ğŸ”— https://lore.kernel.org/kvmarm/20250108091503.12345-4-developer@company.com/
â””â”€ ğŸ“„ [4/4] Update documentation (09:19) [h8i9j0k1]
   ğŸ”— https://lore.kernel.org/kvmarm/20250108091504.12345-5-developer@company.com/
   â””â”€ ğŸ’¬ Re: Minor documentation fix - Contributor (11:00) [i9j0k1l2]
      ğŸ”— https://lore.kernel.org/kvmarm/doc-fix-001@contributor.org/

ğŸ“ˆ AI Analysis Summary:
   ğŸ¯ Purpose: Implement lazy SVE context switching for performance optimization
   âš¡ Impact: 15% performance improvement for non-SVE workloads
   ğŸ”„ Status: Under active review, positive feedback from maintainers
   âš ï¸  Concerns: Need more platform testing, documentation updates pending
   ğŸ“š Technical Depth: High complexity, involves ARM64 architecture specifics

Thread 2: [PATCH v2 1/1] KVM: arm64: Fix memory leak in stage2 mapping
ğŸ”— Thread URL: https://lore.kernel.org/kvmarm/20250109142000.6789-1-maria@contributor.com/T/

ğŸ“„ Single patch - Maria Contributor (2025-01-09 14:20) [j0k1l2m3] ğŸ› ï¸ Bug Fix
ğŸ”— https://lore.kernel.org/kvmarm/20250109142000.6789-1-maria@contributor.com/
â””â”€ âœ… Re: Applied, thanks - Maintainer (2025-01-10 08:00) [k1l2m3n4]
   ğŸ”— https://lore.kernel.org/kvmarm/applied-001@kernel.org/

ğŸ“Š Weekly Statistics:
   ğŸ“§ Messages: 23 total (15 patches, 6 replies, 2 acks)
   ğŸ‘¥ Contributors: 8 unique (3 new contributors this week)
   ğŸ·ï¸  Topics: SVE optimization (major), memory management (minor)
   ğŸ“ˆ Activity: +35% compared to previous week
   ğŸ” Review Quality: High (detailed technical discussions)
   ğŸ”— Lore Links: 23 generated, 21 verified active

ğŸš€ Quick Actions:
   ğŸ“– Browse all threads: https://lore.kernel.org/kvmarm/
   ğŸ” Search ARM KVM: https://lore.kernel.org/kvmarm/?q=arm+kvm
   ğŸ“ˆ Activity trends: https://lore.kernel.org/kvmarm/stats/

ğŸ”— Generated Files:
   ğŸ“„ results/2025-01-15/arm_kvm_threads.json (complete tree with lore links)
   ğŸ“Š results/2025-01-15/arm_kvm_analysis.json (AI analysis + clickable links)
   ğŸ“ˆ results/2025-01-15/weekly_report.md (human-readable summary with URLs)
   ğŸ“‰ results/2025-01-15/chunking_stats.json (processing statistics)
   ğŸŒ results/2025-01-15/lore_links.html (interactive HTML report)
```

## é…ç½®å’Œéƒ¨ç½²

### ç¯å¢ƒå˜é‡
```bash
export OPENAI_API_KEY="your_openai_key"
export GEMINI_API_KEY="your_gemini_key" 
export AI_PROVIDER="openai"  # openai/gemini/claude
export MAX_TOKENS_PER_CHUNK=8000
export WEEKLY_ANALYSIS=true
```

### å¿«é€Ÿå¯åŠ¨ï¼ˆæœ¬åœ°æµ‹è¯•ç‰ˆï¼‰
```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo> arm-kvm-analyzer
cd arm-kvm-analyzer

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo "OPENAI_API_KEY: ${OPENAI_API_KEY:+å·²è®¾ç½®}"

# åˆ†æ­¥æµ‹è¯•ï¼ˆåŒ…å«loreéªŒè¯ï¼‰
python -c "from analyze import test_repository_management; test_repository_management()"
python -c "from analyze import test_email_parsing_with_lore; test_email_parsing_with_lore()"
python -c "from analyze import test_lore_link_validation; test_lore_link_validation()"  # æ–°å¢
python -c "from analyze import test_tree_building; test_tree_building()"
python -c "from analyze import test_content_chunking; test_content_chunking()"
python -c "from analyze import test_ai_analysis; test_ai_analysis()"

# å®Œæ•´æµ‹è¯•ï¼ˆæœ€è¿‘3å¤©çš„é‚®ä»¶ï¼‰
python -c "from analyze import test_full_pipeline; test_full_pipeline()"

# æ­£å¼è¿è¡Œï¼ˆæŒ‡å®šæ—¶é—´èŒƒå›´ï¼‰
python analyze.py --since 2025-01-01 --until 2025-01-15 --debug

# å¿«é€Ÿè¿è¡Œï¼ˆæœ€è¿‘100ä¸ªcommitï¼‰
python analyze.py --mode recent --limit 100
```

### æœ¬åœ°å¼€å‘å»ºè®®
1. **å…ˆè¿è¡Œåˆ†æ­¥æµ‹è¯•**: ç¡®ä¿æ¯ä¸ªç»„ä»¶æ­£å¸¸å·¥ä½œ
2. **å°æ•°æ®é›†éªŒè¯**: ä½¿ç”¨æœ€è¿‘å‡ å¤©çš„é‚®ä»¶æµ‹è¯•
3. **æ£€æŸ¥loreé“¾æ¥**: éªŒè¯ç”Ÿæˆçš„é“¾æ¥å¯ä»¥æ­£å¸¸è®¿é—®
4. **AtoméªŒè¯æµ‹è¯•**: é‡ç‚¹æµ‹è¯•å†…å®¹åŒ¹é…ç®—æ³•çš„å‡†ç¡®æ€§
5. **AIè°ƒç”¨æµ‹è¯•**: ç¡®è®¤OpenAI APIæ­£å¸¸å·¥ä½œ
6. **ç»“æœéªŒè¯**: æ£€æŸ¥ç”Ÿæˆçš„JSONå’Œå¯è§†åŒ–è¾“å‡º
7. **æ€§èƒ½ç›‘æ§**: å…³æ³¨tokenä½¿ç”¨é‡å’Œå¤„ç†æ—¶é—´
8. **åŒ¹é…åº¦åˆ†æ**: æŸ¥çœ‹loreé“¾æ¥éªŒè¯çš„åŒ¹é…åº¦åˆ†å¸ƒ

### GitHub Actionså®ç°è®¡åˆ’
- **Phase 1**: å®Œæˆæœ¬åœ°æµ‹è¯•å’ŒåŠŸèƒ½éªŒè¯
- **Phase 2**: è®¾è®¡GitHub Actions workflow
- **Phase 3**: é…ç½®secretså’Œç¯å¢ƒå˜é‡
- **Phase 4**: æµ‹è¯•è‡ªåŠ¨åŒ–æµç¨‹
- **Phase 5**: éƒ¨ç½²weekly schedule

è¯·åŸºäºä»¥ä¸Šè®¾è®¡å®ç°ä¸€ä¸ªå®Œæ•´çš„ã€å¯è¿­ä»£æµ‹è¯•çš„ARM KVMé‚®ä»¶åˆ—è¡¨åˆ†æç³»ç»Ÿã€‚
