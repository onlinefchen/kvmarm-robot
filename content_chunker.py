import logging
import re
from typing import List, Dict, Optional
import tiktoken

from models import ContentChunk, ChunkType, EmailNode, EmailForest, MessageType
from email_parser import EmailParser
from repository import RepositoryManager


logger = logging.getLogger(__name__)


class ContentChunker:
    def __init__(self, max_tokens: int = 8000):
        self.max_tokens = max_tokens
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        self.repo_manager = RepositoryManager()
        self.email_parser = EmailParser()
        
    def apply_intelligent_chunking(self, forest: EmailForest) -> Dict[str, List[ContentChunk]]:
        """å¯¹é‚®ä»¶æ£®æ—è¿›è¡Œæ™ºèƒ½åˆ†å‰²"""
        chunked_data = {}
        
        for thread in forest.threads:
            for node in thread.all_nodes.values():
                # è·å–é‚®ä»¶çš„å®Œæ•´å†…å®¹
                content = self._extract_full_content(node)
                
                if self._needs_chunking(content):
                    chunks = self._smart_chunk_content(content, node)
                else:
                    # ä¸éœ€è¦åˆ†å‰²ï¼Œåˆ›å»ºå•ä¸ªchunk
                    chunks = [ContentChunk(
                        chunk_id=f"{node.git_hash}_0",
                        git_hash=node.git_hash,
                        chunk_type=ChunkType.HEADER,
                        content=content,
                        priority=5,
                        token_count=self._count_tokens(content)
                    )]
                
                chunked_data[node.message_id] = chunks
                node.content_chunks = chunks
        
        return chunked_data
    
    def _extract_full_content(self, node: EmailNode) -> str:
        """æå–é‚®ä»¶çš„å®Œæ•´å†…å®¹"""
        try:
            # ä»gitä»“åº“è·å–åŸå§‹å†…å®¹
            content = self.repo_manager.get_commit_content(node.git_hash)
            return content
        except Exception as e:
            logger.error(f"Failed to extract content for {node.git_hash}: {e}")
            return ""
    
    def _needs_chunking(self, content: str) -> bool:
        """åˆ¤æ–­å†…å®¹æ˜¯å¦éœ€è¦åˆ†å‰²"""
        token_count = self._count_tokens(content)
        
        # Tokenæ•°é‡è¶…è¿‡é˜ˆå€¼
        if token_count > self.max_tokens:
            return True
        
        # ä»£ç diffè¶…è¿‡10KB
        if self._get_code_diff_size(content) > 10240:
            return True
        
        # å¤šå±‚å›å¤åµŒå¥—è¶…è¿‡3çº§
        if self._get_reply_nesting_level(content) > 3:
            return True
        
        return False
    
    def _smart_chunk_content(self, content: str, node: EmailNode) -> List[ContentChunk]:
        """æ™ºèƒ½åˆ†å‰²å†…å®¹"""
        chunks = []
        
        # 1. åˆ†ç¦»é‚®ä»¶å¤´éƒ¨ä¿¡æ¯
        header_chunk = self._extract_header_chunk(content, node)
        if header_chunk:
            chunks.append(header_chunk)
        
        # 2. æ ¹æ®é‚®ä»¶ç±»å‹è¿›è¡Œä¸åŒçš„åˆ†å‰²ç­–ç•¥
        if node.message_type in [MessageType.PATCH, MessageType.PATCH_COVER]:
            chunks.extend(self._chunk_patch_content(content, node))
        elif node.message_type == MessageType.REVIEW:
            chunks.extend(self._chunk_review_content(content, node))
        else:
            chunks.extend(self._chunk_discussion_content(content, node))
        
        # 3. ç¡®ä¿æ¯ä¸ªchunkéƒ½åœ¨tokené™åˆ¶å†…
        chunks = self._ensure_chunk_size_limits(chunks)
        
        return chunks
    
    def _extract_header_chunk(self, content: str, node: EmailNode) -> Optional[ContentChunk]:
        """æå–é‚®ä»¶å¤´éƒ¨ä¿¡æ¯chunk"""
        # æå–é‚®ä»¶å¤´éƒ¨ï¼ˆFrom, Date, Subjectç­‰ï¼‰
        header_pattern = r'^(From:.*?\n\n)'
        match = re.search(header_pattern, content, re.DOTALL | re.MULTILINE)
        
        if match:
            header_content = match.group(1)
            
            # æ·»åŠ å…³é”®çš„é‚®ä»¶å…ƒæ•°æ®
            header_content += f"\nMessage-Type: {node.message_type.value}\n"
            if node.patch_info:
                header_content += f"Patch: v{node.patch_info.version} "
                header_content += f"{node.patch_info.number}/{node.patch_info.total}\n"
            
            return ContentChunk(
                chunk_id=f"{node.git_hash}_header",
                git_hash=node.git_hash,
                chunk_type=ChunkType.HEADER,
                content=header_content,
                priority=5,
                token_count=self._count_tokens(header_content)
            )
        
        return None
    
    def _chunk_patch_content(self, content: str, node: EmailNode) -> List[ContentChunk]:
        """åˆ†å‰²è¡¥ä¸å†…å®¹"""
        chunks = []
        
        # 1. æå–commit message/summary
        summary_chunk = self._extract_patch_summary(content, node)
        if summary_chunk:
            chunks.append(summary_chunk)
        
        # 2. åˆ†å‰²diffå†…å®¹
        diff_chunks = self._extract_diff_chunks(content, node)
        chunks.extend(diff_chunks)
        
        return chunks
    
    def _chunk_review_content(self, content: str, node: EmailNode) -> List[ContentChunk]:
        """åˆ†å‰²reviewå†…å®¹"""
        chunks = []
        
        # 1. æå–review summary
        summary_chunk = self._extract_review_summary(content, node)
        if summary_chunk:
            chunks.append(summary_chunk)
        
        # 2. åˆ†å‰²è¯¦ç»†è¯„è®º
        comment_chunks = self._extract_review_comments(content, node)
        chunks.extend(comment_chunks)
        
        return chunks
    
    def _chunk_discussion_content(self, content: str, node: EmailNode) -> List[ContentChunk]:
        """åˆ†å‰²è®¨è®ºå†…å®¹"""
        chunks = []
        
        # æŒ‰ç…§å¼•ç”¨å±‚çº§åˆ†å‰²
        sections = self._split_by_quote_level(content)
        
        for i, section in enumerate(sections):
            chunk = ContentChunk(
                chunk_id=f"{node.git_hash}_disc_{i}",
                git_hash=node.git_hash,
                chunk_type=ChunkType.DISCUSSION,
                content=section,
                priority=3,
                token_count=self._count_tokens(section)
            )
            chunks.append(chunk)
        
        return chunks
    
    def _extract_patch_summary(self, content: str, node: EmailNode) -> Optional[ContentChunk]:
        """æå–è¡¥ä¸æ‘˜è¦"""
        # æŸ¥æ‰¾commit messageéƒ¨åˆ†
        commit_msg_pattern = r'\n\n(.*?)(?:\n---\n|\ndiff --git)'
        match = re.search(commit_msg_pattern, content, re.DOTALL)
        
        if match:
            summary = match.group(1).strip()
            return ContentChunk(
                chunk_id=f"{node.git_hash}_summary",
                git_hash=node.git_hash,
                chunk_type=ChunkType.SUMMARY,
                content=summary,
                priority=4,
                token_count=self._count_tokens(summary)
            )
        
        return None
    
    def _extract_diff_chunks(self, content: str, node: EmailNode) -> List[ContentChunk]:
        """æå–å¹¶åˆ†å‰²diffå†…å®¹"""
        chunks = []
        
        # æŸ¥æ‰¾difféƒ¨åˆ†
        diff_pattern = r'(diff --git.*?)(?=diff --git|$)'
        diffs = re.findall(diff_pattern, content, re.DOTALL)
        
        for i, diff in enumerate(diffs):
            # åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®ä»£ç æ›´æ”¹
            is_critical = self._is_critical_code_change(diff)
            chunk_type = ChunkType.CODE_CRITICAL if is_critical else ChunkType.CODE_DETAIL
            priority = 4 if is_critical else 2
            
            chunk = ContentChunk(
                chunk_id=f"{node.git_hash}_diff_{i}",
                git_hash=node.git_hash,
                chunk_type=chunk_type,
                content=diff,
                priority=priority,
                token_count=self._count_tokens(diff)
            )
            chunks.append(chunk)
        
        return chunks
    
    def _extract_review_summary(self, content: str, node: EmailNode) -> Optional[ContentChunk]:
        """æå–reviewæ‘˜è¦"""
        # æŸ¥æ‰¾reviewçš„ä¸»è¦è§‚ç‚¹ï¼ˆé€šå¸¸åœ¨é‚®ä»¶å¼€å¤´ï¼‰
        lines = content.split('\n')
        summary_lines = []
        
        for line in lines:
            if line.strip() and not line.startswith('>'):
                summary_lines.append(line)
                if len('\n'.join(summary_lines)) > 1000:  # é™åˆ¶æ‘˜è¦é•¿åº¦
                    break
        
        if summary_lines:
            summary = '\n'.join(summary_lines)
            return ContentChunk(
                chunk_id=f"{node.git_hash}_review_summary",
                git_hash=node.git_hash,
                chunk_type=ChunkType.SUMMARY,
                content=summary,
                priority=4,
                token_count=self._count_tokens(summary)
            )
        
        return None
    
    def _extract_review_comments(self, content: str, node: EmailNode) -> List[ContentChunk]:
        """æå–reviewçš„è¯¦ç»†è¯„è®º"""
        chunks = []
        
        # æŒ‰ç…§å¼•ç”¨çš„ä»£ç æ®µåˆ†å‰²è¯„è®º
        comment_pattern = r'(>+.*?\n(?:[^>].*?\n)*)'
        comments = re.findall(comment_pattern, content, re.MULTILINE)
        
        for i, comment in enumerate(comments):
            chunk = ContentChunk(
                chunk_id=f"{node.git_hash}_comment_{i}",
                git_hash=node.git_hash,
                chunk_type=ChunkType.DISCUSSION,
                content=comment,
                priority=3,
                token_count=self._count_tokens(comment)
            )
            chunks.append(chunk)
        
        return chunks
    
    def _split_by_quote_level(self, content: str) -> List[str]:
        """æŒ‰å¼•ç”¨å±‚çº§åˆ†å‰²å†…å®¹"""
        sections = []
        current_section = []
        current_quote_level = 0
        
        for line in content.split('\n'):
            # è®¡ç®—å¼•ç”¨å±‚çº§
            quote_level = len(re.match(r'^(>+)', line).group(1)) if line.startswith('>') else 0
            
            # å¦‚æœå¼•ç”¨å±‚çº§å˜åŒ–ï¼Œåˆ›å»ºæ–°section
            if quote_level != current_quote_level and current_section:
                sections.append('\n'.join(current_section))
                current_section = []
                current_quote_level = quote_level
            
            current_section.append(line)
        
        if current_section:
            sections.append('\n'.join(current_section))
        
        return sections
    
    def _ensure_chunk_size_limits(self, chunks: List[ContentChunk]) -> List[ContentChunk]:
        """ç¡®ä¿æ¯ä¸ªchunkéƒ½åœ¨tokené™åˆ¶å†…"""
        final_chunks = []
        
        for chunk in chunks:
            if chunk.token_count <= self.max_tokens:
                final_chunks.append(chunk)
            else:
                # éœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                sub_chunks = self._split_large_chunk(chunk)
                final_chunks.extend(sub_chunks)
        
        return final_chunks
    
    def _split_large_chunk(self, chunk: ContentChunk) -> List[ContentChunk]:
        """åˆ†å‰²è¿‡å¤§çš„chunk"""
        sub_chunks = []
        content = chunk.content
        
        # æŒ‰è¡Œåˆ†å‰²ï¼Œç¡®ä¿ä¸è¶…è¿‡tokené™åˆ¶
        lines = content.split('\n')
        current_content = []
        current_tokens = 0
        sub_index = 0
        
        for line in lines:
            line_tokens = self._count_tokens(line + '\n')
            
            if current_tokens + line_tokens > self.max_tokens and current_content:
                # åˆ›å»ºæ–°çš„sub chunk
                sub_chunk = ContentChunk(
                    chunk_id=f"{chunk.chunk_id}_sub_{sub_index}",
                    git_hash=chunk.git_hash,
                    chunk_type=chunk.chunk_type,
                    content='\n'.join(current_content),
                    priority=chunk.priority,
                    token_count=current_tokens
                )
                sub_chunks.append(sub_chunk)
                
                # é‡ç½®
                current_content = [line]
                current_tokens = line_tokens
                sub_index += 1
            else:
                current_content.append(line)
                current_tokens += line_tokens
        
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_content:
            sub_chunk = ContentChunk(
                chunk_id=f"{chunk.chunk_id}_sub_{sub_index}",
                git_hash=chunk.git_hash,
                chunk_type=chunk.chunk_type,
                content='\n'.join(current_content),
                priority=chunk.priority,
                token_count=current_tokens
            )
            sub_chunks.append(sub_chunk)
        
        return sub_chunks
    
    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        return len(self.tokenizer.encode(text))
    
    def _get_code_diff_size(self, content: str) -> int:
        """è·å–ä»£ç diffçš„å¤§å°"""
        diff_pattern = r'diff --git.*?(?=diff --git|$)'
        diffs = re.findall(diff_pattern, content, re.DOTALL)
        return sum(len(diff.encode('utf-8')) for diff in diffs)
    
    def _get_reply_nesting_level(self, content: str) -> int:
        """è·å–å›å¤åµŒå¥—å±‚çº§"""
        max_level = 0
        for line in content.split('\n'):
            if line.startswith('>'):
                level = len(re.match(r'^(>+)', line).group(1))
                max_level = max(max_level, level)
        return max_level
    
    def _is_critical_code_change(self, diff: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®ä»£ç æ›´æ”¹"""
        # æ£€æŸ¥æ˜¯å¦ä¿®æ”¹äº†å…³é”®æ–‡ä»¶
        critical_patterns = [
            r'arch/arm64/kvm',
            r'virt/kvm/arm',
            r'include/.*kvm',
            r'\.h\s+\|',  # å¤´æ–‡ä»¶æ›´æ”¹
            r'Kconfig',
            r'Makefile'
        ]
        
        for pattern in critical_patterns:
            if re.search(pattern, diff, re.IGNORECASE):
                return True
        
        return False


def apply_intelligent_chunking(forest: EmailForest, max_tokens: int = 8000) -> Dict[str, List[ContentChunk]]:
    """åº”ç”¨æ™ºèƒ½åˆ†å‰²çš„ä¾¿æ·å‡½æ•°"""
    chunker = ContentChunker(max_tokens)
    return chunker.apply_intelligent_chunking(forest)


def test_content_chunking():
    """æµ‹è¯•æ™ºèƒ½å†…å®¹åˆ†å‰²"""
    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½å†…å®¹åˆ†å‰²...")
    
    from email_parser import extract_emails_by_date_range
    from tree_builder import build_email_forest
    from lore_links import LoreLinksManager
    
    # è·å–æµ‹è¯•é‚®ä»¶
    emails = extract_emails_by_date_range(None, limit=20)
    
    if not emails:
        print("   âš ï¸  æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•é‚®ä»¶")
        return
    
    # æ·»åŠ loreé“¾æ¥
    lore_manager = LoreLinksManager()
    for email in emails:
        email.lore_url = lore_manager.generate_lore_url(email.message_id)
    
    # æ„å»ºæ£®æ—
    forest = build_email_forest(emails)
    
    # åº”ç”¨æ™ºèƒ½åˆ†å‰²
    chunked_data = apply_intelligent_chunking(forest, max_tokens=8000)
    
    # ç»Ÿè®¡åˆ†å‰²ç»“æœ
    total_chunks = sum(len(chunks) for chunks in chunked_data.values())
    chunked_emails = sum(1 for chunks in chunked_data.values() if len(chunks) > 1)
    
    print(f"   ğŸ“Š æ€»åˆ†å—æ•°: {total_chunks}")
    print(f"   ğŸ“§ éœ€è¦åˆ†å—çš„é‚®ä»¶: {chunked_emails}")
    
    # æ˜¾ç¤ºä¸€äº›åˆ†å—ç¤ºä¾‹
    for message_id, chunks in list(chunked_data.items())[:3]:
        if len(chunks) > 1:
            print(f"\n   âœ‚ï¸  é‚®ä»¶ {message_id[:40]}... åˆ†æˆ {len(chunks)} å—:")
            for chunk in chunks:
                print(f"      ğŸ“ {chunk.chunk_type.value} (ä¼˜å…ˆçº§: {chunk.priority}, " +
                      f"tokens: {chunk.token_count})")
    
    print("\nâœ… å†…å®¹åˆ†å‰²æµ‹è¯•é€šè¿‡")