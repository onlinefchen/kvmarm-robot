import os
import json
import logging
from typing import Dict, List, Any, Optional
from abc import ABC, abstractmethod
from dotenv import load_dotenv

from models import ContentChunk, EmailForest, ChunkType


logger = logging.getLogger(__name__)
load_dotenv()


class AIProvider(ABC):
    """AIæä¾›å•†çš„æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def analyze_chunk(self, chunk: ContentChunk, context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªå†…å®¹å—"""
        pass
    
    @abstractmethod
    def merge_analyses(self, chunk_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶åˆ†å—åˆ†æç»“æœ"""
        pass


class OpenAIProvider(AIProvider):
    """OpenAI GPT-4 æä¾›å•†"""
    
    def __init__(self, language: str = "zh"):
        try:
            from openai import OpenAI
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            self.model = "gpt-4-turbo-preview"
            self.language = language
        except ImportError:
            logger.error("OpenAI library not installed")
            raise
    
    def analyze_chunk(self, chunk: ContentChunk, context: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨GPT-4åˆ†æå•ä¸ªå†…å®¹å—"""
        prompt = self._build_analysis_prompt(chunk, context)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._get_system_prompt()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            return result
            
        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            return self._get_default_analysis()
    
    def merge_analyses(self, chunk_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶å¤šä¸ªåˆ†æç»“æœ"""
        merge_prompt = self._build_merge_prompt(chunk_analyses)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._get_merge_system_prompt()},
                    {"role": "user", "content": merge_prompt}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            return result
            
        except Exception as e:
            logger.error(f"OpenAI merge error: {e}")
            return self._merge_manually(chunk_analyses)
    
    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        if self.language == "zh":
            return """ä½ æ˜¯ä¸€ä½Linuxå†…æ ¸å¼€å‘ä¸“å®¶ï¼Œä¸“ç²¾äºARMæ¶æ„å’ŒKVMè™šæ‹ŸåŒ–æŠ€æœ¯ã€‚
            è¯·åˆ†ææ¥è‡ªLinux ARM KVMé‚®ä»¶åˆ—è¡¨çš„é‚®ä»¶å†…å®¹ï¼Œå¹¶æä¾›ç»“æ„åŒ–çš„è§è§£ã€‚
            å§‹ç»ˆè¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚æ‰€æœ‰çš„åˆ†æå†…å®¹è¯·ä½¿ç”¨ä¸­æ–‡ã€‚"""
        else:
            return """You are an expert Linux kernel developer specializing in ARM architecture and KVM virtualization.
            Analyze the given email content from the Linux ARM KVM mailing list and provide structured insights.
            Always return valid JSON format."""
    
    def _get_merge_system_prompt(self) -> str:
        """è·å–åˆå¹¶åˆ†æçš„ç³»ç»Ÿæç¤ºè¯"""
        if self.language == "zh":
            return """ä½ æ˜¯æŠ€æœ¯åˆ†æç»¼åˆä¸“å®¶ã€‚
            è¯·å°†æä¾›çš„åˆ†å—åˆ†æåˆå¹¶æˆç»¼åˆæ‘˜è¦ã€‚
            ä¿ç•™å…³é”®æŠ€æœ¯ç»†èŠ‚ï¼ŒåŒæ—¶å»é™¤å†—ä½™å†…å®¹ã€‚
            å§‹ç»ˆè¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚æ‰€æœ‰çš„åˆ†æå†…å®¹è¯·ä½¿ç”¨ä¸­æ–‡ã€‚"""
        else:
            return """You are an expert at synthesizing technical analyses.
            Merge the provided chunk analyses into a comprehensive summary.
            Preserve key technical details while removing redundancy.
            Always return valid JSON format."""
    
    def _build_analysis_prompt(self, chunk: ContentChunk, context: Dict[str, Any]) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        if self.language == "zh":
            return f"""è¯·åˆ†æä»¥ä¸‹ARM KVMé‚®ä»¶åˆ—è¡¨å†…å®¹ï¼š

ä¸»é¢˜: {context.get('thread_subject', 'Unknown')}
å†…å®¹ç±»å‹: {chunk.chunk_type.value}
ä¼˜å…ˆçº§: {chunk.priority}

å†…å®¹:
{chunk.content[:4000]}  # é™åˆ¶å†…å®¹é•¿åº¦

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼æä¾›åˆ†æï¼ˆè¯·ç”¨ä¸­æ–‡å¡«å†™æ‰€æœ‰å†…å®¹ï¼‰ï¼š
{{
    "technical_points": ["å…³é”®æŠ€æœ¯è¦ç‚¹åˆ—è¡¨"],
    "arm_kvm_relevance": 1-10,
    "complexity": "low/medium/high",
    "key_changes": ["é‡è¦å˜æ›´åˆ—è¡¨ï¼ˆå¦‚é€‚ç”¨ï¼‰"],
    "potential_issues": ["æ½œåœ¨é—®é¢˜æˆ–å…³æ³¨ç‚¹åˆ—è¡¨"],
    "innovation_level": "incremental/moderate/significant",
    "summary": "å†…å®¹ç®€è¦æ¦‚è¿°"
}}"""
        else:
            return f"""Analyze this ARM KVM mailing list content:

Thread Subject: {context.get('thread_subject', 'Unknown')}
Content Type: {chunk.chunk_type.value}
Priority: {chunk.priority}

Content:
{chunk.content[:4000]}  # é™åˆ¶å†…å®¹é•¿åº¦

Please provide analysis in the following JSON format:
{{
    "technical_points": ["list of key technical points"],
    "arm_kvm_relevance": 1-10,
    "complexity": "low/medium/high",
    "key_changes": ["list of important changes if applicable"],
    "potential_issues": ["list of potential issues or concerns"],
    "innovation_level": "incremental/moderate/significant",
    "summary": "brief summary of the content"
}}"""
    
    def _build_merge_prompt(self, chunk_analyses: List[Dict[str, Any]]) -> str:
        """æ„å»ºåˆå¹¶æç¤ºè¯"""
        analyses_text = json.dumps(chunk_analyses, indent=2)
        
        if self.language == "zh":
            return f"""è¯·å°†ä»¥ä¸‹åˆ†å—åˆ†æåˆå¹¶ä¸ºç»¼åˆæ‘˜è¦ï¼š

{analyses_text}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼æä¾›åˆå¹¶åˆ†æï¼ˆè¯·ç”¨ä¸­æ–‡å¡«å†™æ‰€æœ‰å†…å®¹ï¼‰ï¼š
{{
    "executive_summary": "æ‰€æœ‰åˆ†å—çš„ç»¼åˆæ‘˜è¦",
    "technical_highlights": ["æ‰€æœ‰åˆ†å—çš„å…³é”®æŠ€æœ¯è¦ç‚¹"],
    "arm_kvm_relevance": 1-10,
    "overall_complexity": "low/medium/high",
    "key_contributions": ["ä¸»è¦è´¡çŒ®æˆ–å˜æ›´"],
    "concerns_raised": ["æåˆ°çš„ä»»ä½•é—®é¢˜æˆ–å…³æ³¨ç‚¹"],
    "recommendation": "å»ºè®®çš„è¡ŒåŠ¨æˆ–å®¡æŸ¥çŠ¶æ€"
}}"""
        else:
            return f"""Merge these chunk analyses into a comprehensive summary:

{analyses_text}

Provide a merged analysis in the following JSON format:
{{
    "executive_summary": "comprehensive summary of all chunks",
    "technical_highlights": ["key technical points across all chunks"],
    "arm_kvm_relevance": 1-10,
    "overall_complexity": "low/medium/high",
    "key_contributions": ["main contributions or changes"],
    "concerns_raised": ["any issues or concerns mentioned"],
    "recommendation": "suggested action or review status"
}}"""
    
    def _get_default_analysis(self) -> Dict[str, Any]:
        """è¿”å›é»˜è®¤åˆ†æç»“æœ"""
        return {
            "technical_points": [],
            "arm_kvm_relevance": 5,
            "complexity": "medium",
            "key_changes": [],
            "potential_issues": [],
            "innovation_level": "incremental",
            "summary": "Analysis failed"
        }
    
    def _merge_manually(self, chunk_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰‹åŠ¨åˆå¹¶åˆ†æç»“æœ"""
        # ç®€å•çš„åˆå¹¶é€»è¾‘
        all_technical_points = []
        all_key_changes = []
        all_issues = []
        relevance_scores = []
        
        for analysis in chunk_analyses:
            all_technical_points.extend(analysis.get("technical_points", []))
            all_key_changes.extend(analysis.get("key_changes", []))
            all_issues.extend(analysis.get("potential_issues", []))
            relevance_scores.append(analysis.get("arm_kvm_relevance", 5))
        
        # å»é‡
        all_technical_points = list(set(all_technical_points))
        all_key_changes = list(set(all_key_changes))
        all_issues = list(set(all_issues))
        
        return {
            "executive_summary": "Merged analysis of multiple chunks",
            "technical_highlights": all_technical_points[:10],  # é™åˆ¶æ•°é‡
            "arm_kvm_relevance": int(sum(relevance_scores) / len(relevance_scores)) if relevance_scores else 5,
            "overall_complexity": "medium",
            "key_contributions": all_key_changes[:5],
            "concerns_raised": all_issues[:5],
            "recommendation": "Requires further review"
        }


class GeminiProvider(AIProvider):
    """Google Gemini æä¾›å•†"""
    
    def __init__(self, language: str = "zh"):
        try:
            import google.generativeai as genai
            genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
            self.model = genai.GenerativeModel('gemini-pro')
            self.language = language
        except ImportError:
            logger.error("Google GenerativeAI library not installed")
            raise
    
    def analyze_chunk(self, chunk: ContentChunk, context: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨Geminiåˆ†æå•ä¸ªå†…å®¹å—"""
        # Geminiå®ç°ç•¥...
        return self._get_default_analysis()
    
    def merge_analyses(self, chunk_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶å¤šä¸ªåˆ†æç»“æœ"""
        # Geminiå®ç°ç•¥...
        return self._merge_manually(chunk_analyses)
    
    def _get_default_analysis(self) -> Dict[str, Any]:
        return {
            "technical_points": [],
            "arm_kvm_relevance": 5,
            "complexity": "medium",
            "key_changes": [],
            "potential_issues": [],
            "innovation_level": "incremental",
            "summary": "Gemini analysis not implemented"
        }
    
    def _merge_manually(self, chunk_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        return {
            "executive_summary": "Gemini merge not implemented",
            "technical_highlights": [],
            "arm_kvm_relevance": 5,
            "overall_complexity": "medium",
            "key_contributions": [],
            "concerns_raised": [],
            "recommendation": "Pending implementation"
        }


class AIAnalyzer:
    """AIåˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self, provider: str = "openai", language: str = "zh"):
        self.provider_name = provider
        self.language = language
        self.provider = self._init_provider(provider)
        self.total_tokens_used = 0
    
    def _init_provider(self, provider: str) -> AIProvider:
        """åˆå§‹åŒ–AIæä¾›å•†"""
        if provider == "openai":
            return OpenAIProvider(self.language)
        elif provider == "gemini":
            return GeminiProvider(self.language)
        else:
            raise ValueError(f"Unsupported AI provider: {provider}")
    
    def analyze_with_ai(self, chunked_data: Dict[str, List[ContentChunk]], forest: EmailForest, debug: bool = False) -> Dict[str, Dict[str, Any]]:
        """å¯¹æ‰€æœ‰å†…å®¹è¿›è¡ŒAIåˆ†æ"""
        all_analyses = {}
        
        print(f"ğŸ¤– ä½¿ç”¨ {self.provider_name} è¿›è¡ŒAIåˆ†æ...")
        
        # éå†æ¯ä¸ªçº¿ç¨‹
        for thread in forest.threads:
            thread_analyses = {}
            
            # ä¸ºçº¿ç¨‹ä¸­çš„æ¯ä¸ªé‚®ä»¶è¿›è¡Œåˆ†æ
            for node in thread.all_nodes.values():
                if node.message_id in chunked_data:
                    chunks = chunked_data[node.message_id]
                    
                    if debug:
                        print(f"   åˆ†æé‚®ä»¶: {node.subject[:50]}... ({len(chunks)} chunks)")
                    
                    # åˆ†ææ¯ä¸ªchunk
                    chunk_analyses = []
                    for chunk in chunks:
                        context = {
                            "thread_subject": thread.subject,
                            "message_type": node.message_type.value,
                            "sender": node.sender
                        }
                        
                        analysis = self.provider.analyze_chunk(chunk, context)
                        chunk_analyses.append(analysis)
                    
                    # åˆå¹¶åˆ†æç»“æœ
                    if len(chunk_analyses) > 1:
                        merged_analysis = self.provider.merge_analyses(chunk_analyses)
                    else:
                        merged_analysis = chunk_analyses[0] if chunk_analyses else {}
                    
                    # ä¿å­˜åˆ†æç»“æœ
                    node.ai_analysis = merged_analysis
                    thread_analyses[node.message_id] = merged_analysis
            
            # ç”Ÿæˆçº¿ç¨‹æ€»ç»“
            if thread_analyses:
                thread_summary = self._generate_thread_summary(thread, thread_analyses)
                thread.ai_summary = thread_summary
                all_analyses[thread.thread_id] = {
                    "thread_summary": thread_summary,
                    "message_analyses": thread_analyses
                }
        
        return all_analyses
    
    def _generate_thread_summary(self, thread, thread_analyses: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆçº¿ç¨‹çº§åˆ«çš„æ€»ç»“"""
        # æ”¶é›†æ‰€æœ‰é‚®ä»¶çš„åˆ†æ
        all_summaries = []
        all_contributions = []
        all_concerns = []
        
        for analysis in thread_analyses.values():
            if "summary" in analysis:
                all_summaries.append(analysis["summary"])
            elif "executive_summary" in analysis:
                all_summaries.append(analysis["executive_summary"])
            
            all_contributions.extend(analysis.get("key_contributions", []))
            all_contributions.extend(analysis.get("key_changes", []))
            all_concerns.extend(analysis.get("concerns_raised", []))
            all_concerns.extend(analysis.get("potential_issues", []))
        
        # æ„å»ºçº¿ç¨‹æ€»ç»“
        return {
            "executive_summary": " ".join(all_summaries[:3]),  # å–å‰3ä¸ªæ€»ç»“
            "technical_highlights": list(set(all_contributions))[:10],
            "discussion_points": list(set(all_concerns))[:10],
            "thread_status": self._determine_thread_status(thread),
            "lore_discussion_url": f"{thread.root_node.lore_url}T/",
            "impact_level": self._assess_impact_level(thread_analyses)
        }
    
    def _determine_thread_status(self, thread) -> str:
        """åˆ¤æ–­çº¿ç¨‹çŠ¶æ€"""
        if thread.statistics.acks > 0:
            return "acked"
        elif thread.statistics.reviews > 0:
            return "under_review"
        elif thread.statistics.replies > 0:
            return "active_discussion"
        else:
            return "new"
    
    def _assess_impact_level(self, thread_analyses: Dict[str, Dict[str, Any]]) -> str:
        """è¯„ä¼°å½±å“çº§åˆ«"""
        relevance_scores = []
        for analysis in thread_analyses.values():
            if "arm_kvm_relevance" in analysis:
                relevance_scores.append(analysis["arm_kvm_relevance"])
        
        if not relevance_scores:
            return "medium"
        
        avg_relevance = sum(relevance_scores) / len(relevance_scores)
        
        if avg_relevance >= 8:
            return "high"
        elif avg_relevance >= 5:
            return "medium"
        else:
            return "low"


def analyze_with_ai(chunked_data: Dict[str, List[ContentChunk]], forest: EmailForest, provider: str = "openai", language: str = "zh", debug: bool = False) -> Dict[str, Dict[str, Any]]:
    """AIåˆ†æçš„ä¾¿æ·å‡½æ•°"""
    analyzer = AIAnalyzer(provider, language)
    return analyzer.analyze_with_ai(chunked_data, forest, debug)


def test_ai_analysis():
    """æµ‹è¯•AIåˆ†æåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•AIåˆ†æ...")
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        return
    
    from models import ContentChunk, ChunkType
    
    # åˆ›å»ºæµ‹è¯•chunk
    test_chunk = ContentChunk(
        chunk_id="test_001",
        git_hash="test_hash",
        chunk_type=ChunkType.HEADER,
        content="""[PATCH v2 1/3] KVM: arm64: Fix memory management issue
        
This patch fixes a critical memory leak in the stage-2 page table
management code. The issue occurs when unmapping large memory regions
during VM teardown.

The fix ensures proper reference counting and cleanup of intermediate
page table levels.""",
        priority=5,
        token_count=50
    )
    
    try:
        analyzer = AIAnalyzer("openai")
        
        context = {
            "thread_subject": "KVM: arm64: Memory management fixes",
            "message_type": "patch",
            "sender": "test@example.com"
        }
        
        result = analyzer.provider.analyze_chunk(test_chunk, context)
        
        # éªŒè¯è¿”å›æ ¼å¼
        assert "technical_points" in result
        assert "arm_kvm_relevance" in result
        assert isinstance(result["arm_kvm_relevance"], (int, float))
        
        print("   âœ… AIåˆ†æè¿”å›æ­£ç¡®æ ¼å¼")
        print(f"   ğŸ“Š ARM KVMç›¸å…³æ€§: {result.get('arm_kvm_relevance', 'N/A')}")
        print(f"   ğŸ’¡ æŠ€æœ¯è¦ç‚¹: {', '.join(result.get('technical_points', [])[:3])}")
        
    except Exception as e:
        print(f"   âŒ AIåˆ†æå¤±è´¥: {e}")
        return
    
    print("\nâœ… AIåˆ†ææµ‹è¯•é€šè¿‡")